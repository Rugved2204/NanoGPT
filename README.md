• Leveraged the concepts from the paper ”Attention is all you need” to build a Generative Pretrained Transformer.
• Incorporated advance concepts such as attention layers and residual connections to enhance performance and capture
intricate linguistic patterns.
• Trained the model on a sizable dataset extracted from William Shakespeare’s Coriolanus, employing a Bigram model
with an extensive vocabulary
